{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Gqe0k-3OEVb-","ry5VtnmoEfsK","TB89Z0Y2FKd2","jD02Qpx4GIZy","1uJaAzo8K-RG","mNdhEXTZLC2m","iXV6VKMpNFyT","6oGAwBkfRMWG"],"authorship_tag":"ABX9TyNnnkLArsVSeJKSiZznTHrz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Go project\n","Auteur du notebook : Roc de Larouzière"],"metadata":{"id":"mdosWWNZX29O"}},{"cell_type":"markdown","source":["**INSTALLATIONS ET IMPORTS** des dépendances nécessaires du projet"],"metadata":{"id":"CUmgzvBDXs22"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwM_ktnyXaNd"},"outputs":[],"source":["!rm -rf /content/sample_data/\n","!rm -f /content/*\n","\n","!wget https://www.lamsade.dauphine.fr/~cazenave/project2025.zip\n","!unzip project2025.zip\n","!ls -l\n","\n","!pip install tensorrt-bindings==8.6.1\n","#!pip install --extra-index-url https://pypi.nvidia.com tensorrt-libs\n","#!pip install tensorflow[and-cuda]==2.15.0\n","!pip install tensorflow==2.15.0\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import layers as L, regularizers, Model\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.optimizers.schedules import CosineDecay\n","import gc\n","import matplotlib.pyplot as plt\n","import logging\n","import csv\n","import golois"]},{"cell_type":"markdown","source":["**TRAITEMENT DES DONNEES**"],"metadata":{"id":"J9idg1JsYaZs"}},{"cell_type":"code","source":["planes = 31      #nombre de plans/features pour représenter l'état du jeu\n","moves = 361      #tous les coups possibles sur un plateau de Go : 19*19 = 361 intersections\n","N = 10000        # Nombre d'exemples d'entraînement\n","filters = 32     #nombre de filtres (ou noyaux) des couches convolutives\n","\n","input_data = np.random.randint(2, size=(N, 19, 19, planes))    #crée un tableau NumPy à 4 dimensions avec des valeurs aléatoires de 0 ou 1.\n","input_data = input_data.astype ('float32') #convertit les int en float\n","\n","policy = np.random.randint(moves, size=(N,))  #tableau 1D de taille 10000 où chaque élément i correspond au coup à jouer pour le ième exemple\n","policy = keras.utils.to_categorical (policy)  #Convertit chaque nombre en un vecteur \"one-hot encoding\", Le résultat final sera un tableau de taille (N, moves) soit (10000, 361)\n","#policy représente les coups à jouer pour chaque exemple, on a donc 10000 vecteurs de taille 361 avec 1.0 pour le coup joué et 0 pour tout le reste\n","#Utilisé pour entrainer le réseau à apprendre le prochain coup\n","\n","value = np.random.randint(2, size=(N,))\n","value = value.astype ('float32')\n","#Évaluation binaire pour chaque position (qui a l'avantage/qui va gagner pour chaque exemple)\n","\n","\n","end = np.random.randint(2, size=(N, 19, 19, 2))\n","end = end.astype ('float32')\n","#État final du plateau pour chaque partie\n","#2 plans probablement pour territoires finales noir (1 plan) et blanc (1 plan)\n","\n","\n","groups = np.zeros((N, 19, 19, 1))\n","groups = groups.astype ('float32')\n","#Information sur les groupes de pierres\n","#Initialisé à zéro, rempli pendant l'entraînement\n","\n","\n","print (\"getValidation\", flush = True)\n","golois.getValidation (input_data, policy, value, end)\n","#Appelle la fonction getValidation du module golois pour préparer les données de validation\n","#Charge ou genere un fichier validation.data, contenant les indices des positions à utiliser pour la validation\n","\n","#Cette partie du code initialise toutes les structures de données nécessaires pour l'entraînement\n","#du réseau de neurones pour le jeu de Go et prépare les données de validation."],"metadata":{"id":"meNw4X91YdS2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PARTIE SUR LES RESIDUAL NETWORKS"],"metadata":{"id":"Gqe0k-3OEVb-"}},{"cell_type":"markdown","source":["## Modèle ResNetSanSe"],"metadata":{"id":"ry5VtnmoEfsK"}},{"cell_type":"code","source":["def create_res_block(x, filters, block_index):\n","    kernel_initializer = tf.keras.initializers.GlorotNormal(seed=42)\n","\n","\n","    residual = x\n","\n","    # First convolutional layer in the residual block\n","    x = L.Conv2D(filters, 3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name=f'res_conv1_block{block_index}')(x)\n","    x = L.BatchNormalization(name=f'res_bn1_block{block_index}')(x)\n","    x = L.ReLU(name=f'res_relu1_block{block_index}')(x)\n","\n","    # Second convolutional layer in the residual block\n","    x = L.Conv2D(filters, 3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name=f'res_conv2_block{block_index}')(x)\n","    x = L.BatchNormalization(name=f'res_bn2_block{block_index}')(x)\n","\n","    # Adding the residual connection\n","    x = L.Add(name=f'res_add_block{block_index}')([x, residual])\n","\n","    x = L.ReLU(name=f'res_relu2_block{block_index}')(x)\n","\n","    return x\n","\n","def create_network_SanSe(input_shape, num_resblocks, num_hidden, action_size):\n","    inputs = L.Input(shape=input_shape, name='input_layer')\n","\n","    #Initial convolutional layer\n","    x = L.Conv2D(num_hidden, 3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name='start_conv')(inputs)\n","    x = L.BatchNormalization(name='start_bn')(x)\n","    x = L.ReLU(name='start_relu')(x)\n","\n","    # Residual blocks\n","    for i in range(num_resblocks):\n","        x = create_res_block(x, num_hidden, block_index=i)\n","\n","    # Policy head\n","    policy = L.Conv2D(32, 3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name='policy_conv1')(x)  # 32 filters\n","    policy = L.BatchNormalization(name='policy_bn1')(policy)\n","    policy = L.ReLU(name='policy_relu1')(policy)\n","    policy = L.Conv2D(1, 1, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name='policy_conv2')(policy)  # Reduce to 1 filter\n","    policy = L.BatchNormalization(name='policy_bn2')(policy)\n","    policy = L.ReLU(name='policy_relu2')(policy)\n","    policy = L.Flatten(name='policy_flatten')(policy)\n","    policy = L.Activation('softmax', name='policy')(policy)  # Output: action probabilities\n","\n","    # Value head\n","    value = L.Conv2D(3, 3, padding='same', name='value_conv1')(x)  # 3 filters\n","    value = L.BatchNormalization(name='value_bn1')(value)\n","    value = L.ReLU(name='value_relu1')(value)\n","    value = L.GlobalAveragePooling2D()(value)\n","    value = L.Dense(50, activation='relu',\n","                              kernel_regularizer=regularizers.l2(1e-4), name='value_dense1')(value)\n","    value = L.Dense(1, activation='sigmoid',\n","                         kernel_regularizer=keras.regularizers.l2(1e-4), name='value')(value)  # Output: scalar value\n","\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=[policy, value], name='Go_Network')\n","\n","    return model"],"metadata":{"id":"UCvzZUzta8xr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compilation du modèle"],"metadata":{"id":"Ykuj-LV_IWHx"}},{"cell_type":"code","source":["input_shape = (19,19,planes)\n","num_resblocks = 4\n","num_hidden = filters   #num_hidden = nombre de filtres (canaux de sortie)\n","action_size = moves\n","\n","\n","model_ResNetSanSe = create_network_SanSe(input_shape, num_resblocks, num_hidden, action_size)\n","model_ResNetSanSe.summary()\n","\n","epochs = 300\n","batch = 64\n","\n","#Cosine Annealing\n","initial_learning_rate = 0.001  # Peut être augmenté (0.01) si nécessaire\n","total_steps = epochs * (N // batch)  # 10 * (10_000//64) ≈ 1560\n","alpha = 0.1  # LR final = 0.1% du LR initial\n","\n","# --- Définition du scheduler ---\n","lr_schedule = CosineDecay(\n","    initial_learning_rate,\n","    total_steps,\n","    alpha=alpha\n",")\n","\n","\n","model_ResNetSanSe.compile(optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule),\n","              loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n","              loss_weights={'policy' : 1.0, 'value' : 1.0},\n","              metrics={\n","                'policy': ['categorical_accuracy'],\n","                'value': ['mse'] })"],"metadata":{"id":"nsWAqX3MHdbB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ResNet2"],"metadata":{"id":"TB89Z0Y2FKd2"}},{"cell_type":"code","source":["def create_res_block(x, filters, block_index):  #Meme bloc residuel que sur le papier de recherche ResNet (cf rapport)\n","    residual = x\n","    x = L.Conv2D(filters, 3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name=f'res_conv1_block{block_index}')(x)\n","    x = L.Add(name=f'res_add_block{block_index}')([x, residual])\n","    x = L.ReLU(name=f'res_relu2_block{block_index}')(x)\n","    x = L.BatchNormalization(name=f'res_bn1_block{block_index}')(x)\n","    return x\n","\n","\n","\n","def create_network_ResNet2(input_shape, num_resblocks, num_hidden, action_size):\n","    inputs = L.Input(shape=input_shape, name='input_layer')\n","\n","    # Initial convolutional layer\n","    x1 = L.Conv2D(num_hidden, 5, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name='start_conv_5')(inputs)\n","    x2 = L.Conv2D(num_hidden, 1, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name='start_conv_1')(inputs)\n","    x = L.Add(name=f'input_add_layer')([x1, x2])\n","    x = L.ReLU(name='start_relu')(x)\n","\n","    # Residual blocks\n","    for i in range(num_resblocks):\n","        x = create_res_block(x, num_hidden, block_index=i)\n","\n","\n","    # Policy head\n","    policy = L.Conv2D(32, 3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name='policy_conv1')(x)  # 32 filters\n","    policy = L.BatchNormalization(name='policy_bn1')(policy)\n","    policy = L.ReLU(name='policy_relu1')(policy)\n","    policy = L.Conv2D(1, 1, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), name='policy_conv2')(policy)  # Reduce to 1 filter\n","    policy = L.BatchNormalization(name='policy_bn2')(policy)\n","    policy = L.ReLU(name='policy_relu2')(policy)\n","    policy = L.Flatten(name='policy_flatten')(policy)\n","    policy = L.Activation('softmax', name='policy')(policy)  # Output: action probabilities\n","\n","    # Value head\n","    value = L.Conv2D(3, 3, padding='same', name='value_conv1')(x)  # 3 filters\n","    value = L.BatchNormalization(name='value_bn1')(value)\n","    value = L.ReLU(name='value_relu1')(value)\n","    value = L.Flatten(name='value_flatten')(value)\n","    value = L.Dense(1, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(1e-4), name='value')(value)  # Output: scalar value\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=[policy, value], name='ResNet_Network')\n","\n","    return model"],"metadata":{"id":"fjGs_apQFRgW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Computation"],"metadata":{"id":"-mrps3VII0lR"}},{"cell_type":"code","source":["model_ResNet2 = create_network_ResNet2(input_shape, num_resblocks, num_hidden, action_size)\n","model_ResNet2.summary()\n","\n","epochs = 300\n","batch = 64\n","\n","#Cosine Annealing\n","initial_learning_rate = 0.001  # Peut être augmenté (0.01) si nécessaire\n","total_steps = epochs * (N // batch)  # 10 * (10_000//64) ≈ 1560\n","alpha = 0.1  # LR final = 0.1% du LR initial\n","\n","# --- Définition du scheduler ---\n","lr_schedule = CosineDecay(\n","    initial_learning_rate,\n","    total_steps,\n","    alpha=alpha\n",")\n","\n","\n","model_ResNet2.compile(optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule),\n","              loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n","              loss_weights={'policy' : 1.0, 'value' : 1.0},\n","              metrics={\n","        'policy': ['categorical_accuracy'],\n","        'value': ['mse'] })"],"metadata":{"id":"NPwmt3V0I4_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training\n","Ici changer model par le nom du modèle que l'on souhaite entrainer"],"metadata":{"id":"jD02Qpx4GIZy"}},{"cell_type":"code","source":["# Fichier CSV pour stocker les pertes et métriques de validation\n","csv_filename = \"save_metrics.csv\"\n","\n","# Initialisation des listes de stockage des métriques\n","policy_losses = []\n","value_losses = []\n","val_policy_accuracy = []\n","val_value_mse = []\n","\n","# Boucle d'entraînement\n","for i in range(1, epochs + 1):\n","    print(f\"Epoch {i}\")\n","\n","    golois.getBatch(input_data, policy, value, end, groups, i * N)\n","\n","    history = model.fit(input_data,\n","                         {'policy': policy, 'value': value},\n","                         epochs=1, batch_size=batch)\n","\n","    #si on tourne en local, décommenter cette partie et commenter la partie au dessus\n","    #dataset = tf.data.Dataset.from_tensor_slices((input_data, {'policy': policy, 'value': value}))\n","    #dataset = dataset.shuffle(N).batch(batch).prefetch(tf.data.AUTOTUNE)\n","    #history = model.fit(dataset, epochs=1, callbacks=[early_stopping])\n","\n","\n","    policy_losses.append(history.history['policy_loss'][0])\n","    value_losses.append(history.history['value_loss'][0])\n","\n","    # Libération de mémoire tous les 5 epochs\n","    if i % 5 == 0:\n","        gc.collect()\n","\n","\n","    if i % 20 == 0:\n","        golois.getValidation(input_data, policy, value, end)\n","        val = model.evaluate(input_data, [policy, value], verbose=0, batch_size=batch)\n","\n","        # Stockage des résultats de validation\n","        val_policy_accuracy.append(val[3])  # categorical_accuracy\n","        val_value_mse.append(val[4])  # mse\n","\n","        # Enregistrement dans le log\n","        log_message = f\"Validation at epoch {i}: {val}\"\n","        print(log_message)\n","\n","    #ici juste pour dire que si on est pas en validation, on met des 0 au lieu de rien mettre\n","    if not (i%20==0) :\n","        val_policy_accuracy.append(0)  # categorical_accuracy\n","        val_value_mse.append(0)  # mse\n","\n","\n","# Sauvegarde des métriques dans un fichier CSV à la fin du training\n","with open(csv_filename, mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow([\"Epoch\", \"Policy Loss\", \"Value Loss\", \"Validation Policy Accuracy\", \"Validation Value MSE\"])\n","    for epoch in range(len(policy_losses)):\n","        writer.writerow([\n","            epoch + 1,\n","            policy_losses[epoch],\n","            value_losses[epoch],\n","            val_policy_accuracy[epoch],\n","            val_value_mse[epoch]\n","        ])\n","\n","# Sauvegarde du modèle entraîné\n","model_filename = \"model.h5\"\n","model.save(model_filename)\n","print(f\"Modèle sauvegardé sous {model_filename}\")"],"metadata":{"id":"0uwy83p-HAYP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PARTIE SUR EFFICIENT FORMER"],"metadata":{"id":"1uJaAzo8K-RG"}},{"cell_type":"markdown","source":["## Modèle EfficientFormerRough"],"metadata":{"id":"mNdhEXTZLC2m"}},{"cell_type":"code","source":["def conv_stem(x, embed_dim): #bloc stem de convolution\n","    x = L.Conv2D(embed_dim, 3, padding='same', use_bias=False)(x)\n","    x = L.BatchNormalization()(x)\n","    x = L.ReLU()(x)\n","    return x\n","\n","def mb4d(x, dim, expansion=4):  #bloc qui s'inspire de mb4d\n","    hidden_dim = dim * expansion\n","    residual = x\n","    x = L.Conv2D(hidden_dim, 1, padding='same', use_bias=False)(x)\n","    x = L.BatchNormalization()(x)\n","    x = L.ReLU()(x)\n","    x = L.DepthwiseConv2D(3, padding='same', use_bias=False)(x)\n","    x = L.BatchNormalization()(x)\n","    x = L.ReLU()(x)\n","    x = L.Conv2D(dim, 1, padding='same', use_bias=False)(x)\n","    x = L.BatchNormalization()(x)\n","    x = L.add([x, residual])\n","    return x\n","\n","def mb3d(x, dim):  #bloc qui s'inspire de mb3d, integrant la MHSA\n","    # Extraction de la forme dynamique\n","    b = tf.shape(x)[0]\n","    h = tf.shape(x)[1]\n","    w = tf.shape(x)[2]\n","    c = tf.shape(x)[3]\n","\n","    x_flat = tf.reshape(x, [b, h * w, c]) #passage en 3 dim\n","    attn_out = L.MultiHeadAttention(num_heads=2, key_dim=dim // 2)(x_flat, x_flat)\n","    x = L.LayerNormalization(epsilon=1e-6)(attn_out + x_flat)\n","    x = L.Dense(dim)(x)\n","    x = tf.reshape(x, [b, h, w, dim]) # Reshape en format image en 4 dim pour refaire la mise a plat etc...\n","    return x\n","\n","def build_fixed_model_Rough(input_shape=(19, 19, 31)):\n","    inputs = L.Input(shape=input_shape)\n","\n","    x = conv_stem(inputs, 32)\n","\n","    # 2 blocs MB4D\n","    x = mb4d(x, 32, expansion=4)\n","    x = mb4d(x, 32, expansion=4)\n","\n","    # Transition\n","    x = L.Conv2D(56, 3, padding='same')(x)\n","    x = L.BatchNormalization()(x)\n","    x = L.ReLU()(x)\n","\n","    # 2 blocs MB3D\n","    x = mb3d(x, 56)\n","    x = mb3d(x, 56)\n","\n","    # Policy head\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False)(x)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.ReLU()(policy_head)\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False)(policy_head)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.ReLU()(policy_head)\n","    policy_head = L.Conv2D(1, 1, activation='relu', padding='same',\n","                             use_bias=False,\n","                             kernel_regularizer=regularizers.l2(0.0001))(policy_head)\n","    policy_head = L.Flatten()(policy_head)\n","    policy_head = L.Activation('softmax', name='policy')(policy_head)\n","\n","    # Value head\n","    value_head = L.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', use_bias=False)(x)\n","    value_head = L.BatchNormalization()(value_head)\n","    value_head = L.Activation(\"swish\")(value_head)\n","    value_head = L.GlobalAveragePooling2D()(value_head)\n","    value_head = L.Dense(50, activation='relu',\n","                         kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","    value_head = L.Dense(1, activation='sigmoid', name='value',\n","                         kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","\n","    model = keras.Model(inputs=inputs, outputs=[policy_head, value_head])\n","    return model"],"metadata":{"id":"FI4mK8bYLKgZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Computation"],"metadata":{"id":"w7eA9WjgMscz"}},{"cell_type":"code","source":["model_EFRough = build_fixed_model_Rough()\n","model_EFRough.summary()\n","\n","epochs = 1000\n","batch = 64\n","\n","#Cosine Annealing\n","initial_learning_rate = 0.001  # Peut être augmenté (0.01) si nécessaire\n","total_steps = epochs * (N // batch)  # 10 * (10_000//64) ≈ 1560\n","alpha = 0.1  # LR final = 0.1% du LR initial\n","\n","# --- Définition du scheduler ---\n","lr_schedule = CosineDecay(\n","    initial_learning_rate,\n","    total_steps,\n","    alpha=alpha\n",")\n","\n","\n","model_EFRough.compile(optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule),\n","              loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n","              loss_weights={'policy' : 1.0, 'value' : 1.0},\n","              metrics={\n","        'policy': ['categorical_accuracy'],\n","        'value': ['mse'] })"],"metadata":{"id":"UodSqwxkMuYM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modèle EfficientFormerWay"],"metadata":{"id":"iXV6VKMpNFyT"}},{"cell_type":"code","source":["def conv_stem(x, out_channels):   #on utilise un simple Conv 3x3 et stride=1 + BN + ReLU pour garder la même résolution (19x19)\n","    x = L.Conv2D(out_channels, kernel_size=3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False, name='stem_conv_3x3')(x)\n","    x = L.BatchNormalization(name='stem_bn')(x)\n","    x = L.ReLU(name='stem_relu')(x)\n","    return x\n","\n","\n","def mb4d(x, dim, expansion, block_id):\n","    # Connexion résiduelle\n","    shortcut = x\n","\n","    # Token mixer local : un pooling 2D simple pour mélanger localement\n","    # puis addition résiduelle\n","    pool = L.AveragePooling2D(pool_size=3, strides=1, padding='same', name=f'mb4d_block{block_id}_avgpool')(x)\n","    x = L.Add(name=f'mb4d_block{block_id}_local_mix')([pool, x])  # local mixing\n","\n","    # MLP convolutionnel\n","    x = L.Conv2D(dim * expansion, kernel_size=1, padding='same', use_bias=False, name=f'mb4d_block{block_id}_expand_conv')(x) #Ici on augmente la dimension avec dim * expansion\n","    x = L.BatchNormalization(name=f'mb4d_block{block_id}_expand_bn')(x)\n","    x = L.ReLU(name=f'mb4d_block{block_id}_expand_relu')(x)\n","\n","    x = L.Conv2D(dim, kernel_size=1, padding='same', use_bias=False, name=f'mb4d_block{block_id}_reduce_conv')(x)   #Ici on réduit à la dimension d'origine\n","    x = L.BatchNormalization(name=f'mb4d_block{block_id}_reduce_bn')(x)\n","\n","    # Ajout de la connexion résiduelle\n","    x = L.Add(name=f'mb4d_block{block_id}_skip')([shortcut, x])\n","    x = L.ReLU(name=f'mb4d_block{block_id}_out')(x)\n","    return x\n","\n","\n","def mb3d(x, dim, num_heads, mlp_expansion, block_id):\n","\n","    shortcut = x\n","\n","    # On récupère (B, H, W, C)\n","    b = tf.shape(x)[0]\n","    h = tf.shape(x)[1]\n","    w = tf.shape(x)[2]\n","    c = tf.shape(x)[3]\n","\n","    # Passage en (B, H*W, C)\n","    x_reshaped = tf.reshape(x, [b, h*w, c])\n","\n","    # LayerNorm + MHSA\n","    x_ln = L.LayerNormalization(name=f'mb3d_block{block_id}_ln_attn')(x_reshaped)\n","    attn_out = L.MultiHeadAttention(num_heads=num_heads, key_dim=dim // num_heads,\n","                                    dropout=0.0, name=f'mb3d_block{block_id}_mhsa')(x_ln, x_ln)\n","\n","    # Skip connection\n","    x = L.Add(name=f'mb3d_block{block_id}_skip_attn')([attn_out, x_reshaped])\n","\n","    # MLP : Dense + GeLU + Dense\n","    x_ln2 = L.LayerNormalization(name=f'mb3d_block{block_id}_ln_ffn')(x)\n","    x_mlp = L.Dense(int(dim * mlp_expansion), use_bias=False, kernel_regularizer=regularizers.l2(1e-4), name=f'mb3d_block{block_id}_ffn_expand')(x_ln2)\n","    x_mlp = L.Activation('gelu', name=f'mb3d_block{block_id}_ffn_gelu')(x_mlp)\n","    x_mlp = L.Dense(dim, use_bias=False, kernel_regularizer=regularizers.l2(1e-4),name=f'mb3d_block{block_id}_ffn_reduce')(x_mlp)\n","\n","    x = L.Add(name=f'mb3d_block{block_id}_skip_ffn')([x, x_mlp])\n","\n","    # On redonne la forme (B, H, W, C) pour revenir au debut du block ensuite\n","    x = tf.reshape(x, [b, h, w, dim], name=f'mb3d_block{block_id}_out')\n","\n","    return x\n","\n","\n","\n","def build_efficientformer_go_Way(input_shape=(19, 19, 31),stem_dim=32, blocks_4d=3, blocks_3d=2, trans_dim=48, num_heads=3, conv_expansion=4, trans_expansion=1.5):\n","\n","    inputs = L.Input(shape=input_shape)\n","\n","    # Embedding initial\n","    x = conv_stem(inputs, stem_dim)\n","\n","\n","    for i in range(blocks_4d):\n","        x = mb4d(x, dim=stem_dim, expansion=conv_expansion, block_id=i)\n","\n","    # Transition vers dimension plus large pour la phase Transformer\n","    x = L.Conv2D(trans_dim, kernel_size=3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = L.BatchNormalization()(x)\n","    x = L.ReLU()(x)\n","\n","    for i in range(blocks_3d):\n","        x = mb3d(x, dim=trans_dim, num_heads=num_heads, mlp_expansion=trans_expansion, block_id=i)\n","\n","\n","\n","    # Policy head\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.ReLU()(policy_head)\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(policy_head)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.ReLU()(policy_head)\n","    policy_head = L.Conv2D(filters=1, kernel_size=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(policy_head)\n","    policy_head = L.Flatten()(policy_head)\n","    policy_out = L.Activation('softmax', name='policy')(policy_head)\n","\n","\n","    # Value head\n","    value_head = L.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.0001))(x)\n","    value_head = L.BatchNormalization()(value_head)\n","    value_head = L.Activation(\"swish\")(value_head)\n","    value_head = L.GlobalAveragePooling2D()(value_head)\n","    value_head = L.Dense(42, activation='relu',\n","                              kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","    value_out = L.Dense(1, activation='sigmoid', name='value',\n","                              kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","\n","\n","    model = keras.Model(inputs=inputs, outputs=[policy_out, value_out], name='efficientformer_go')\n","    return model"],"metadata":{"id":"G3VtYsxSNWir"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Computation"],"metadata":{"id":"YcNbiFm8OozG"}},{"cell_type":"code","source":["model_EFWay = build_efficientformer_go_Way()\n","model_EFWay.summary()\n","\n","\n","\n","epochs = 1000\n","batch = 64\n","\n","#Cosine Annealing\n","initial_learning_rate = 0.001  # Peut être augmenté (0.01) si nécessaire\n","total_steps = epochs * (N // batch)  # 10 * (10_000//64) ≈ 1560\n","alpha = 0.1  # LR final = 0.1% du LR initial (0.001 * 0.1 = 0.0001)\n","\n","# --- Définition du scheduler ---\n","lr_schedule = CosineDecay(\n","    initial_learning_rate,\n","    total_steps,\n","    alpha=alpha\n",")\n","\n","\n","model_EFWay.compile(optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule),\n","              loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n","              loss_weights={'policy' : 1.0, 'value' : 1.0},\n","              metrics={\n","        'policy': ['categorical_accuracy'],\n","        'value': ['mse'] })\n"],"metadata":{"id":"O1u8PBX1Oogt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modèle GOat"],"metadata":{"id":"ZV4UJi8dPLz-"}},{"cell_type":"code","source":["def conv_stem(x, out_channels): #comme au dessus\n","    x = L.Conv2D(out_channels, kernel_size=3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False, name='stem_conv_3x3')(x)\n","    x = L.BatchNormalization(name='stem_bn')(x)\n","    x = L.Activation('swish',name='stem_relu')(x)\n","    return x\n","\n","\n","def mixconv2d(x, kernels=[3,5], name=\"mixconv\"):  #Applique deux depthwise convs (k=3 et k=5) en parallèle et concatène leur sortie\n","\n","    input_channels = x.shape[-1]\n","    num_groups = len(kernels)\n","    channels_per_group = input_channels // num_groups\n","\n","    group_outputs = []\n","    for i, k in enumerate(kernels):\n","        # Sélectionne un des groupes de canaux\n","        x_group = L.Lambda(lambda z: z[:,:,:, i*channels_per_group:(i+1)*channels_per_group])(x)\n","\n","        # Convolution depthwise\n","        x_group = L.DepthwiseConv2D(\n","            kernel_size=k,\n","            strides=1,\n","            padding='same',\n","            use_bias=False,\n","            kernel_regularizer=regularizers.l2(1e-4),\n","            name=f\"{name}_dw{k}_{i}\"\n","        )(x_group)\n","\n","        # On recolle le résultat de ce groupe\n","        group_outputs.append(x_group)\n","\n","    # On concatène le tout\n","    out = L.Concatenate(axis=-1, name=f\"{name}_concat\")(group_outputs)\n","    return out\n","\n","\n","def mb4d(x, dim, expansion, block_id):\n","\n","\n","    shortcut = x\n","\n","    # Token mixing\n","    pool = mixconv2d(x, kernels=[3,5], name=f'mb4d_block{block_id}_mixconv')\n","    x = L.Add(name=f'mb4d_block{block_id}_local_mix')([pool, x])  # local mixing\n","\n","    # MLP convolutionnel\n","    x = L.Conv2D(dim * expansion, kernel_size=1, padding='same', use_bias=False, name=f'mb4d_block{block_id}_expand_conv')(x) #Ici on augmente la dimension avec dim * expansion\n","    x = L.BatchNormalization(name=f'mb4d_block{block_id}_expand_bn')(x)\n","    x = L.Activation('swish', name=f'mb4d_block{block_id}_expand_swish')(x)\n","\n","    x = L.Conv2D(dim, kernel_size=1, padding='same', use_bias=False, name=f'mb4d_block{block_id}_reduce_conv')(x)   #Ici on réduit à la dimension d'origine\n","    x = L.BatchNormalization(name=f'mb4d_block{block_id}_reduce_bn')(x)\n","\n","    # Ajout de la connexion résiduelle\n","    x = L.Add(name=f'mb4d_block{block_id}_skip')([shortcut, x])\n","    x = L.Activation('swish', name=f'mb4d_block{block_id}_out')(x)\n","    return x\n","\n","\n","def mb3d(x, dim, num_heads, mlp_expansion, block_id):\n","\n","    shortcut = x\n","\n","    # On récupère (B, H, W, C)\n","    b = tf.shape(x)[0]\n","    h = tf.shape(x)[1]\n","    w = tf.shape(x)[2]\n","    c = tf.shape(x)[3]\n","\n","    # Passage en (B, H*W, C)\n","    x_reshaped = tf.reshape(x, [b, h*w, c])\n","\n","    # LayerNorm + MHSA\n","    x_ln = L.LayerNormalization(name=f'mb3d_block{block_id}_ln_attn')(x_reshaped)\n","    attn_out = L.MultiHeadAttention(num_heads=num_heads, key_dim=dim // num_heads,\n","                                    dropout=0.0, name=f'mb3d_block{block_id}_mhsa')(x_ln, x_ln)\n","\n","    # Skip-connection\n","    x = L.Add(name=f'mb3d_block{block_id}_skip_attn')([attn_out, x_reshaped])\n","\n","    # MLP : Dense + GeLU + Dense\n","    x_ln2 = L.LayerNormalization(name=f'mb3d_block{block_id}_ln_ffn')(x)\n","    x_mlp = L.Dense(int(dim * mlp_expansion), use_bias=False, kernel_regularizer=regularizers.l2(1e-4), name=f'mb3d_block{block_id}_ffn_expand')(x_ln2)\n","    x_mlp = L.Activation('gelu', name=f'mb3d_block{block_id}_ffn_gelu')(x_mlp)\n","    x_mlp = L.Dense(dim, use_bias=False, kernel_regularizer=regularizers.l2(1e-4),name=f'mb3d_block{block_id}_ffn_reduce')(x_mlp)\n","\n","    x = L.Add(name=f'mb3d_block{block_id}_skip_ffn')([x, x_mlp])\n","\n","    # On redonne la forme (B, H, W, C)\n","    x = tf.reshape(x, [b, h, w, dim], name=f'mb3d_block{block_id}_out')\n","\n","    return x\n","\n","\n","\n","def build_efficientformer_GOat(input_shape=(19, 19, 31),stem_dim=32, blocks_4d=3, blocks_3d=2, trans_dim=48, num_heads=3, conv_expansion=4, trans_expansion=1.5):\n","\n","    inputs = L.Input(shape=input_shape)\n","\n","    # Embedding initial : conv stem\n","    x = conv_stem(inputs, stem_dim)\n","\n","\n","    x = mb4d(x, dim=stem_dim, expansion=conv_expansion, block_id=0)\n","    x = mb4d(x, dim=stem_dim, expansion=conv_expansion, block_id=1)\n","    x = mb4d(x, dim=stem_dim, expansion=conv_expansion-1, block_id=2)\n","\n","\n","    x = L.Conv2D(trans_dim, kernel_size=3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    x = L.BatchNormalization()(x)\n","    x = L.Activation('swish')(x)\n","\n","\n","    for i in range(blocks_3d):\n","        x = mb3d(x, dim=trans_dim, num_heads=num_heads, mlp_expansion=trans_expansion, block_id=i)\n","\n","\n","\n","    # Policy head\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.Activation('swish')(policy_head)\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(policy_head)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.Activation('swish')(policy_head)\n","    policy_head = L.Conv2D(filters=1, kernel_size=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(policy_head)\n","    policy_head = L.Flatten()(policy_head)\n","    policy_out = L.Activation('softmax', name='policy')(policy_head)\n","\n","\n","    # Value head\n","    value_head = L.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.0001))(x)\n","    value_head = L.BatchNormalization()(value_head)\n","    value_head = L.Activation(\"swish\")(value_head)\n","    value_head = L.GlobalAveragePooling2D()(value_head)\n","    value_head = L.Dense(42, activation='swish',\n","                              kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","\n","    value_out = L.Dense(1, activation='sigmoid', name='value',\n","                              kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","\n","\n","    model = keras.Model(inputs=inputs, outputs=[policy_out, value_out], name='efficientformer_go')\n","    return model"],"metadata":{"id":"zeXTc5gdPRfx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Computation"],"metadata":{"id":"PAh30GdBPYGa"}},{"cell_type":"code","source":["# On construit le modèle et on regarde le résumé\n","model_GOat = build_efficientformer_GOat()\n","model_GOat.summary()\n","\n","\n","\n","epochs = 900\n","batch = 64\n","\n","#Cosine Annealing\n","initial_learning_rate = 0.001  # Peut être augmenté (0.01) si nécessaire\n","total_steps = epochs * (N // batch)  # 10 * (10_000//64) ≈ 1560\n","alpha = 0.1  # LR final = 0.1% du LR initial (0.001 * 0.1 = 0.0001)\n","\n","# --- Définition du scheduler ---\n","lr_schedule = CosineDecay(\n","    initial_learning_rate,\n","    total_steps,\n","    alpha=alpha\n",")\n","\n","\n","model_GOat.compile(optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule),\n","              loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n","              loss_weights={'policy' : 1.0, 'value' : 1.0},\n","              metrics={\n","        'policy': ['categorical_accuracy'],\n","        'value': ['mse'] })"],"metadata":{"id":"JyMDE8M4PXyX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training\n","Ici changer model par le nom du modèle que l'on souhaite entrainer"],"metadata":{"id":"wDidIYM8R0DH"}},{"cell_type":"code","source":["# Fichier CSV pour stocker les pertes et métriques de validation\n","csv_filename = \"save_metrics.csv\"\n","\n","# Initialisation des listes de stockage des métriques\n","policy_losses = []\n","value_losses = []\n","val_policy_accuracy = []\n","val_value_mse = []\n","\n","# Boucle d'entraînement\n","for i in range(1, epochs + 1):\n","    print(f\"Epoch {i}\")\n","\n","    golois.getBatch(input_data, policy, value, end, groups, i * N)\n","\n","    history = model.fit(input_data,\n","                         {'policy': policy, 'value': value},\n","                         epochs=1, batch_size=batch)\n","\n","    #si on tourne en local, décommenter cette partie et commenter la partie au dessus\n","    #dataset = tf.data.Dataset.from_tensor_slices((input_data, {'policy': policy, 'value': value}))\n","    #dataset = dataset.shuffle(N).batch(batch).prefetch(tf.data.AUTOTUNE)\n","    #history = model.fit(dataset, epochs=1, callbacks=[early_stopping])\n","\n","\n","    policy_losses.append(history.history['policy_loss'][0])\n","    value_losses.append(history.history['value_loss'][0])\n","\n","    # Libération de mémoire tous les 5 epochs\n","    if i % 5 == 0:\n","        gc.collect()\n","\n","\n","    if i % 20 == 0:\n","        golois.getValidation(input_data, policy, value, end)\n","        val = model.evaluate(input_data, [policy, value], verbose=0, batch_size=batch)\n","\n","        # Stockage des résultats de validation\n","        val_policy_accuracy.append(val[3])  # categorical_accuracy\n","        val_value_mse.append(val[4])  # mse\n","\n","        # Enregistrement dans le log\n","        log_message = f\"Validation at epoch {i}: {val}\"\n","        print(log_message)\n","\n","    #ici juste pour dire que si on est pas en validation, on met des 0 au lieu de rien mettre\n","    if not (i%20==0) :\n","        val_policy_accuracy.append(0)  # categorical_accuracy\n","        val_value_mse.append(0)  # mse\n","\n","\n","# Sauvegarde des métriques dans un fichier CSV à la fin du training\n","with open(csv_filename, mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow([\"Epoch\", \"Policy Loss\", \"Value Loss\", \"Validation Policy Accuracy\", \"Validation Value MSE\"])\n","    for epoch in range(len(policy_losses)):\n","        writer.writerow([\n","            epoch + 1,\n","            policy_losses[epoch],\n","            value_losses[epoch],\n","            val_policy_accuracy[epoch],\n","            val_value_mse[epoch]\n","        ])\n","\n","# Sauvegarde du modèle entraîné\n","model_filename = \"model.h5\"\n","model.save(model_filename)\n","print(f\"Modèle sauvegardé sous {model_filename}\")"],"metadata":{"id":"Sm75w3KkR2uO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PARTIE SUR MOBILENETSV3"],"metadata":{"id":"6oGAwBkfRMWG"}},{"cell_type":"markdown","source":["## Modele MobileNet3_A"],"metadata":{"id":"gP0E8CzUSFOG"}},{"cell_type":"code","source":["# Activation HardSwish\n","def h_swish(x):\n","    return x * tf.nn.relu6(x + 3) / 6\n","\n","# Activation HardSigmoid\n","def h_sigmoid(x):\n","    return tf.nn.relu6(x + 3) / 6\n","\n","\n","# Squeeze-and-Excite function\n","def squeeze_excite(x, se_ratio=4):\n","    input_channels = x.shape[-1]\n","    se = L.GlobalAveragePooling2D()(x)  # Pooling global pour obtenir une représentation (batch, channels)\n","\n","    se = L.Reshape((1, 1, input_channels))(se)  # Reshape pour avoir une carte de caractéristiques 1x1\n","    reduced_channels = input_channels // se_ratio\n","\n","    # Première projection (équivalent à une convolution 1x1)\n","    se = L.Conv2D(reduced_channels, kernel_size=1, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(se)\n","    se = L.BatchNormalization()(se)\n","    se = L.ReLU()(se)\n","\n","    # Expansion pour revenir au nombre de canaux initial\n","    se = L.Conv2D(input_channels, kernel_size=1, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(se)\n","    se = L.BatchNormalization()(se)\n","    se = h_sigmoid(se)\n","\n","    # Multiplication élément par élément avec l'entrée\n","    return L.Multiply()([x, se])\n","\n","\n","\n","def mixconv2d(x, kernels=[3,5], name=\"mixconv\"):\n","    input_channels = x.shape[-1]\n","    num_groups = len(kernels)\n","    channels_per_group = input_channels // num_groups\n","\n","    group_outputs = []\n","    for i, k in enumerate(kernels):\n","        x_group = L.Lambda(lambda z: z[:,:,:, i*channels_per_group:(i+1)*channels_per_group])(x)\n","\n","        x_group = L.DepthwiseConv2D(\n","            kernel_size=k,\n","            strides=1,\n","            padding='same',\n","            use_bias=False,\n","            kernel_regularizer=regularizers.l2(1e-4)\n","        )(x_group)\n","\n","\n","        group_outputs.append(x_group)\n","\n","\n","    out = L.Concatenate(axis=-1)(group_outputs)\n","    return out\n","\n","\n","# Bloc Bottleneck (MobileNetV3)\n","def bottleneck(x, kernel, stride, expansion, output_channels, activation, se=False):\n","    input_tensor = x  # sauvegarde pour la connexion résiduelle\n","\n","    # Phase d'expansion\n","    x = L.Conv2D(expansion, kernel_size=1, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(x)\n","    x = L.BatchNormalization()(x)\n","    x = activation(x)\n","\n","    # Convolution depthwise\n","    x = mixconv2d(x, kernels=[3, 5])\n","    x = L.BatchNormalization()(x)\n","    x = activation(x)\n","\n","    # Squeeze-and-Excite si activé\n","    if se:\n","        x = squeeze_excite(x, se_ratio=4)\n","\n","    # Convolution pointwise\n","    x = L.Conv2D(output_channels, kernel_size=1, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(x)\n","    x = L.BatchNormalization()(x)\n","    x = activation(x)\n","\n","    # Connexion résiduelle : si le stride est 1 et les dimensions correspondent\n","    if stride == 1:\n","        if input_tensor.shape[-1] == output_channels:\n","            x = L.Add()([x, input_tensor])\n","        else:\n","            shortcut = L.Conv2D(output_channels, kernel_size=1, strides=stride, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(input_tensor)\n","            shortcut = L.BatchNormalization()(shortcut)\n","            x = L.Add()([x, shortcut])\n","    else:\n","        # Si stride > 1, on ajuste la dimension du raccourci\n","        shortcut = L.Conv2D(output_channels, kernel_size=1, strides=stride, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(input_tensor)\n","        shortcut = L.BatchNormalization()(shortcut)\n","        x = L.Add()([x, shortcut])\n","\n","    return x\n","\n","\n","# Construction du modèle MobileNetV3\n","def MobileNetV3_A(input_shape):\n","    inputs = L.Input(shape=input_shape)\n","\n","    # Convolution initiale\n","    x = L.Conv2D(32, kernel_size=3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(inputs)\n","    x = L.BatchNormalization()(x)\n","    x = h_swish(x)\n","\n","\n","\n","    x = bottleneck(x, kernel=3, stride=1, expansion=32, output_channels=32,\n","                   activation=tf.keras.activations.swish, se=False)\n","\n","\n","    x = bottleneck(x, kernel=3, stride=1, expansion=32, output_channels=32,\n","                   activation=tf.keras.activations.swish, se=False)\n","\n","\n","    x = bottleneck(x, kernel=3, stride=1, expansion=64, output_channels=64,\n","                   activation=h_swish, se=True)\n","\n","\n","    x = bottleneck(x, kernel=3, stride=1, expansion=128, output_channels=128,\n","                   activation=h_swish, se=True)\n","\n","\n","\n","    # Policy head\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.Activation('swish')(policy_head)\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(policy_head)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.Activation('swish')(policy_head)\n","    policy_head = L.Conv2D(filters=1, kernel_size=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(policy_head)\n","    policy_head = L.Flatten()(policy_head)\n","    policy_out = L.Activation('softmax', name='policy')(policy_head)\n","\n","    # Value head\n","    value_head = L.Conv2D(filters=16, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.0001))(x)\n","    value_head = L.BatchNormalization()(value_head)\n","    value_head = L.Activation(\"swish\")(value_head)\n","    value_head = L.GlobalAveragePooling2D()(value_head)\n","    value_head = L.Dense(50, activation='swish',\n","                              kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","\n","    value_out = L.Dense(1, activation='sigmoid', name='value',\n","                              kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","\n","\n","    model = keras.Model(inputs=inputs, outputs=[policy_out, value_out], name='MobileNet_go')\n","\n","    return model"],"metadata":{"id":"b22_hURRSVK6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Computation"],"metadata":{"id":"SY3hZWTXTLiU"}},{"cell_type":"code","source":["model_3A = MobileNetV3_A(input_shape=(19, 19, 31))\n","model_3A.summary()\n","\n","\n","\n","epochs = 800\n","batch = 64\n","\n","#Cosine Annealing\n","initial_learning_rate = 0.001  # Peut être augmenté (0.01) si nécessaire\n","total_steps = epochs * (N // batch)  # 10 * (10_000//64) ≈ 1560\n","alpha = 0.1  # LR final = 0.1% du LR initial (0.001 * 0.1 = 0.0001)\n","\n","# --- Définition du scheduler ---\n","lr_schedule = CosineDecay(\n","    initial_learning_rate,\n","    total_steps,\n","    alpha=alpha\n",")\n","\n","\n","model_3A.compile(optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule),\n","              loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n","              loss_weights={'policy' : 1.0, 'value' : 1.0},\n","              metrics={\n","        'policy': ['categorical_accuracy'],\n","        'value': ['mse'] })\n"],"metadata":{"id":"2DA1Q5uZTM-S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modele MobileNet3_B"],"metadata":{"id":"ZTjZ0ySOTcSN"}},{"cell_type":"code","source":["# Hard Swish\n","def h_swish(x):\n","    return x * tf.nn.relu6(x + 3) / 6\n","\n","# Hard Sigmoid\n","def h_sigmoid(x):\n","    return tf.nn.relu6(x + 3) / 6\n","\n","\n","# Définition de l'attention canal\n","def channel_attention(input_feature, ratio=8):\n","    channel = input_feature.shape[-1]\n","\n","    # Pooling moyen et max sur l'espace (H, W)\n","    avg_pool = L.GlobalAveragePooling2D()(input_feature)\n","    max_pool = L.GlobalMaxPooling2D()(input_feature)\n","\n","    # On reforme pour avoir (batch, 1, 1, channels)\n","    avg_pool = L.Reshape((1, 1, channel))(avg_pool)\n","    max_pool = L.Reshape((1, 1, channel))(max_pool)\n","\n","    shared_dense_one = L.Dense(channel // ratio,\n","                               activation='relu',\n","                               kernel_initializer='he_normal',\n","                               use_bias=True,\n","                               kernel_regularizer=keras.regularizers.l2(1e-4),\n","                               bias_initializer='zeros')\n","    shared_dense_two = L.Dense(channel,\n","                               kernel_initializer='he_normal',\n","                               use_bias=True,\n","                               kernel_regularizer=keras.regularizers.l2(1e-4),\n","                               bias_initializer='zeros')\n","\n","    avg_out = shared_dense_two(shared_dense_one(avg_pool))\n","    max_out = shared_dense_two(shared_dense_one(max_pool))\n","\n","    added = L.Add()([avg_out, max_out])\n","    added = L.Dense(24, activation='relu', kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.l2(1e-4))(added)\n","    added = L.Dense(channel, activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.l2(1e-4))(added)\n","\n","    return L.Multiply()([input_feature, added])\n","\n","\n","# Définition de l'attention spatiale\n","def spatial_attention(input_feature):\n","    # On calcule le pooling moyen et max sur les canaux\n","    avg_pool = tf.reduce_mean(input_feature, axis=-1, keepdims=True)\n","    max_pool = tf.reduce_max(input_feature, axis=-1, keepdims=True)\n","    concat = L.Concatenate(axis=-1)([avg_pool, max_pool])\n","\n","    # Convolution pour générer la carte d'attention spatiale\n","    cbam_feature = L.Conv2D(filters=1,\n","                            kernel_size=7,\n","                            strides=1,\n","                            padding='same',\n","                            activation='sigmoid',\n","                            kernel_initializer='he_normal',\n","                            kernel_regularizer=keras.regularizers.l2(1e-4),\n","                            use_bias=False)(concat)\n","    return L.Multiply()([input_feature, cbam_feature])\n","\n","\n","# Bloc CBAM\n","def cbam_block(input_feature, ratio=8):\n","    x = channel_attention(input_feature, ratio)\n","    x = spatial_attention(x)\n","    return x\n","\n","\n","\n","def mixconv2d(x, kernels=[3,5]):\n","    input_channels = x.shape[-1]\n","    num_groups = len(kernels)\n","    channels_per_group = input_channels // num_groups\n","\n","    group_outputs = []\n","    for i, k in enumerate(kernels):\n","\n","        # Sélectionne un \"groupe\" de canaux\n","        x_group = L.Lambda(lambda z: z[:,:,:, i*channels_per_group:(i+1)*channels_per_group])(x)\n","\n","        # Convolution depthwise\n","        x_group = L.DepthwiseConv2D(\n","            kernel_size=k,\n","            strides=1,\n","            padding='same',\n","            use_bias=False,\n","            kernel_regularizer=regularizers.l2(1e-4)\n","        )(x_group)\n","\n","        # On recolle le résultat de ce groupe\n","        group_outputs.append(x_group)\n","\n","    # On concatène le tout\n","    out = L.Concatenate(axis=-1)(group_outputs)\n","    return out\n","\n","\n","# Bloc Bottleneck\n","def bottleneck(x, kernel, stride, expansion, output_channels, activation, se=False):\n","    input_tensor = x\n","\n","    # Phase d'expansion\n","    x = L.Conv2D(expansion, kernel_size=1, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(x)\n","    x = L.BatchNormalization()(x)\n","    x = activation(x)\n","\n","    # Convolution depthwise\n","    x = mixconv2d(x, kernels=[3, 5])\n","    x = L.BatchNormalization()(x)\n","    x = activation(x)\n","\n","    # Squeeze-and-Excite remplacé par CBAM\n","    if se:\n","        x = cbam_block(x, ratio=8)\n","\n","    # Convolution pointwise\n","    x = L.Conv2D(output_channels, kernel_size=1, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(x)\n","    x = L.BatchNormalization()(x)\n","    x = activation(x)\n","\n","    # Connexion résiduelle : si le stride est 1 et les dimensions correspondent\n","    if stride == 1:\n","        if input_tensor.shape[-1] == output_channels:\n","            x = L.Add()([x, input_tensor])\n","        else:\n","            shortcut = L.Conv2D(output_channels, kernel_size=1, strides=stride, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(input_tensor)\n","            shortcut = L.BatchNormalization()(shortcut)\n","            x = L.Add()([x, shortcut])\n","    else:\n","        # Si stride > 1, on ajuste la dimension du raccourci\n","        shortcut = L.Conv2D(output_channels, kernel_size=1, strides=stride, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(input_tensor)\n","        shortcut = L.BatchNormalization()(shortcut)\n","        x = L.Add()([x, shortcut])\n","\n","    return x\n","\n","\n","\n","def MobileNetV3_B(input_shape):\n","    inputs = L.Input(shape=input_shape)\n","\n","\n","    x = L.Conv2D(32, kernel_size=3, padding='same', kernel_regularizer=keras.regularizers.l2(1e-4), use_bias=False)(inputs)\n","    x = L.BatchNormalization()(x)\n","    x = h_swish(x)\n","\n","\n","\n","    x = bottleneck(x, kernel=3, stride=1, expansion=32, output_channels=32,\n","                   activation=tf.keras.activations.swish, se=False)\n","\n","\n","    x = bottleneck(x, kernel=3, stride=1, expansion=32, output_channels=32,\n","                   activation=tf.keras.activations.swish, se=True)\n","\n","\n","    x = bottleneck(x, kernel=3, stride=1, expansion=64, output_channels=64,\n","                   activation=h_swish, se=False)\n","\n","\n","    x = bottleneck(x, kernel=3, stride=1, expansion=128, output_channels=128,\n","                   activation=h_swish, se=True)\n","\n","\n","\n","\n","    # Policy head\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(x)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.Activation('swish')(policy_head)\n","    policy_head = L.Conv2D(4, 3, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(policy_head)\n","    policy_head = L.BatchNormalization()(policy_head)\n","    policy_head = L.Activation('swish')(policy_head)\n","    policy_head = L.Conv2D(filters=1, kernel_size=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(1e-4))(policy_head)\n","    policy_head = L.Flatten()(policy_head)\n","    policy_out = L.Activation('softmax', name='policy')(policy_head)\n","\n","\n","    # Value head\n","    value_head = L.Conv2D(filters=16, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.0001))(x)\n","    value_head = L.BatchNormalization()(value_head)\n","    value_head = L.Activation(\"swish\")(value_head)\n","    value_head = L.GlobalAveragePooling2D()(value_head)\n","    value_head = L.Dense(50, activation='swish',\n","                              kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","\n","    value_out = L.Dense(1, activation='sigmoid', name='value',\n","                              kernel_regularizer=regularizers.l2(0.0001))(value_head)\n","\n","\n","    model = keras.Model(inputs=inputs, outputs=[policy_out, value_out], name='MobileNet_go')\n","\n","    return model"],"metadata":{"id":"aTkUBQ_ITiCs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["computation"],"metadata":{"id":"ed33iu2FTiRA"}},{"cell_type":"code","source":["model_3B = MobileNetV3_B(input_shape=(19, 19, 31))\n","model_3B.summary()\n","\n","\n","epochs = 800\n","batch = 64\n","\n","#Cosine Annealing\n","initial_learning_rate = 0.001  # Peut être augmenté (0.01) si nécessaire\n","total_steps = epochs * (N // batch)  # 10 * (10_000//64) ≈ 1560\n","alpha = 0.1  # LR final = 0.1% du LR initial (0.001 * 0.1 = 0.0001)\n","\n","# --- Définition du scheduler ---\n","lr_schedule = CosineDecay(\n","    initial_learning_rate,\n","    total_steps,\n","    alpha=alpha\n",")\n","\n","\n","model_3B.compile(optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule),\n","              loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n","              loss_weights={'policy' : 1.0, 'value' : 1.0},\n","              metrics={\n","        'policy': ['categorical_accuracy'],\n","        'value': ['mse'] })"],"metadata":{"id":"TpFE8NglTkB_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training\n","Ici changer model par le nom du modèle que l'on souhaite entrainer"],"metadata":{"id":"umKm9-5vUZPs"}},{"cell_type":"code","source":["# Fichier CSV pour stocker les pertes et métriques de validation\n","csv_filename = \"save_metrics.csv\"\n","\n","# Initialisation des listes de stockage des métriques\n","policy_losses = []\n","value_losses = []\n","val_policy_accuracy = []\n","val_value_mse = []\n","\n","# Boucle d'entraînement\n","for i in range(1, epochs + 1):\n","    print(f\"Epoch {i}\")\n","\n","    golois.getBatch(input_data, policy, value, end, groups, i * N)\n","\n","    history = model.fit(input_data,\n","                         {'policy': policy, 'value': value},\n","                         epochs=1, batch_size=batch)\n","\n","    #si on tourne en local, décommenter cette partie et commenter la partie au dessus\n","    #dataset = tf.data.Dataset.from_tensor_slices((input_data, {'policy': policy, 'value': value}))\n","    #dataset = dataset.shuffle(N).batch(batch).prefetch(tf.data.AUTOTUNE)\n","    #history = model.fit(dataset, epochs=1, callbacks=[early_stopping])\n","\n","\n","    policy_losses.append(history.history['policy_loss'][0])\n","    value_losses.append(history.history['value_loss'][0])\n","\n","    # Libération de mémoire tous les 5 epochs\n","    if i % 5 == 0:\n","        gc.collect()\n","\n","\n","    if i % 20 == 0:\n","        golois.getValidation(input_data, policy, value, end)\n","        val = model.evaluate(input_data, [policy, value], verbose=0, batch_size=batch)\n","\n","        # Stockage des résultats de validation\n","        val_policy_accuracy.append(val[3])  # categorical_accuracy\n","        val_value_mse.append(val[4])  # mse\n","\n","        # Enregistrement dans le log\n","        log_message = f\"Validation at epoch {i}: {val}\"\n","        print(log_message)\n","\n","    #ici juste pour dire que si on est pas en validation, on met des 0 au lieu de rien mettre\n","    if not (i%20==0) :\n","        val_policy_accuracy.append(0)  # categorical_accuracy\n","        val_value_mse.append(0)  # mse\n","\n","\n","# Sauvegarde des métriques dans un fichier CSV à la fin du training\n","with open(csv_filename, mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow([\"Epoch\", \"Policy Loss\", \"Value Loss\", \"Validation Policy Accuracy\", \"Validation Value MSE\"])\n","    for epoch in range(len(policy_losses)):\n","        writer.writerow([\n","            epoch + 1,\n","            policy_losses[epoch],\n","            value_losses[epoch],\n","            val_policy_accuracy[epoch],\n","            val_value_mse[epoch]\n","        ])\n","\n","# Sauvegarde du modèle entraîné\n","model_filename = \"model.h5\"\n","model.save(model_filename)\n","print(f\"Modèle sauvegardé sous {model_filename}\")"],"metadata":{"id":"Y3kmleTJUfqr"},"execution_count":null,"outputs":[]}]}